{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 18:06:12 [scrapy.utils.log] INFO: Scrapy 2.13.1 started (bot: scrapybot)\n",
      "2025-06-01 18:06:12 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '5.4.0',\n",
      " 'libxml2': '2.11.9',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '24.11.0',\n",
      " 'Python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, '\n",
      "           '18:49:16) [MSC v.1929 64 bit (AMD64)]',\n",
      " 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',\n",
      " 'cryptography': '45.0.3',\n",
      " 'Platform': 'Windows-11-10.0.26100-SP0'}\n",
      "2025-06-01 18:06:12 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-06-01 18:06:12 [scrapy.extensions.telnet] INFO: Telnet Password: 67bc538ed77212e3\n",
      "2025-06-01 18:06:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-06-01 18:06:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2025-06-01 18:06:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-06-01 18:06:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-06-01 18:06:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReactorNotRestartable\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     41\u001b[39m process = CrawlerProcess(settings={\n\u001b[32m     42\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mUSER_AGENT\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mChrome/97.0\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     43\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLOG_LEVEL\u001b[39m\u001b[33m'\u001b[39m: logging.INFO,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     }\n\u001b[32m     47\u001b[39m })\n\u001b[32m     49\u001b[39m process.crawl(BookingSpider)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# /html/body/table[3]/tbody/tr/td[2]/table[3]/tbody/tr[2]/td[1]\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# /html/body/table[3]/tbody/tr/td[2]/table[3]/tbody/tr[2]/td[2]\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# /html/body/table[3]/tbody/tr/td[2]/table[3]/tbody/tr[2]/td[3]\u001b[39;00m\n\u001b[32m     54\u001b[39m html/body/table[\u001b[32m3\u001b[39m]/tbody/tr/td[\u001b[32m2\u001b[39m]/table[\u001b[32m3\u001b[39m]/tbody/tr[\u001b[32m2\u001b[39m]/td[\u001b[32m2\u001b[39m]/font()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m_bar\\Incendie\\.venv\\Lib\\site-packages\\scrapy\\crawler.py:502\u001b[39m, in \u001b[36mCrawlerProcess.start\u001b[39m\u001b[34m(self, stop_after_crawl, install_signal_handlers)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[32m    499\u001b[39m     reactor.addSystemEventTrigger(\n\u001b[32m    500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mafter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstartup\u001b[39m\u001b[33m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m._signal_shutdown\n\u001b[32m    501\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m \u001b[43mreactor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m_bar\\Incendie\\.venv\\Lib\\site-packages\\twisted\\internet\\asyncioreactor.py:252\u001b[39m, in \u001b[36mAsyncioSelectorReactor.run\u001b[39m\u001b[34m(self, installSignalHandlers)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncioEventloop.run_forever()\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._justStopped:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\m_bar\\Incendie\\.venv\\Lib\\site-packages\\twisted\\internet\\base.py:926\u001b[39m, in \u001b[36mReactorBase.startRunning\u001b[39m\u001b[34m(self, installSignalHandlers)\u001b[39m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.ReactorAlreadyRunning()\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._startedBefore:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.ReactorNotRestartable()\n\u001b[32m    928\u001b[39m \u001b[38;5;28mself\u001b[39m._signals.uninstall()\n\u001b[32m    929\u001b[39m \u001b[38;5;28mself\u001b[39m._installSignalHandlers = installSignalHandlers\n",
      "\u001b[31mReactorNotRestartable\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 18:06:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-06-01 18:06:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-06-01 18:06:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-06-01 18:06:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-06-01 18:06:12 [scrapy.extensions.feedexport] INFO: Stored json feed (419 items) in: scrapping/corse_insee.json\n",
      "2025-06-01 18:06:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 225,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 142261,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.471759,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 6, 1, 16, 6, 12, 996779, tzinfo=datetime.timezone.utc),\n",
      " 'item_scraped_count': 419,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'responses_per_minute': None,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2025, 6, 1, 16, 6, 12, 525020, tzinfo=datetime.timezone.utc)}\n",
      "2025-06-01 18:06:13 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Cela permet d’exécuter une boucle asyncio imbriquée dans Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class BookingSpider(scrapy.Spider):\n",
    "    name = \"code_insee_corse\"\n",
    "    start_urls = [\n",
    "        'http://geneatouque.free.fr/infodept/codcom/codcom20.html'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        rows = response.xpath('//table[3]//tr[position()>1]')  # ignore l'entête\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.xpath('./td')\n",
    "            \n",
    "            if len(cols) >= 3:\n",
    "                code_postal = cols[0].xpath('./p//text()').get(default='').strip()\n",
    "                nom_commune = cols[1].xpath('.//text()').get(default='').strip()\n",
    "                code_insee = cols[2].xpath('./p//text()').get(default='').strip()\n",
    "\n",
    "                yield {\n",
    "                    'code_postale': code_postal or \"Pas de code postal\",\n",
    "                    'nom_de_la_commune': nom_commune or \"Pas de nom de la commune\",\n",
    "                    'code_insee': code_insee or \"Pas de code INSEE\",\n",
    "                }\n",
    "\n",
    "# Chemin du fichier\n",
    "filename = \"corse_insee.json\"\n",
    "output_path = \"scrapping/\" + filename\n",
    "\n",
    "# Supprimer l’ancien fichier si nécessaire\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "# Lancer Scrapy\n",
    "process = CrawlerProcess(settings={\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    'FEEDS': {\n",
    "        output_path: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BookingSpider)\n",
    "process.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
